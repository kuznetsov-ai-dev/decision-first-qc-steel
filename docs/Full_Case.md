### Triage под ограниченный ресурс QC

## Оглавление

1. Введение
2. Бизнес-контекст и проблема
3. Decision framing
4. Почему обычные ML-метрики не подходят
5. Бизнес-метрики и ограничения
6. Данные и sanity-check
7. Технические особенности данных
8. Стратегия валидации
9. Baseline модели (ML-слой)
10. Критичность как гипотеза
11. Выбор основного сценария
12. Policy и risk-scoring
13. Результаты triage
14. Cost-thinking и роль K
15. Проверка гипотез улучшений
16. Error analysis
17. Feature importance и интерпретация
18. Production-мышление
19. Итог кейса
20. Почему этот кейс “дорогой”

---

## 0. Введение: зачем вообще этот кейс

Этот кейс — **не про обучить модель** и не про гонку за метриками.
Он про то, **как нужно принять решение**, когда:

* есть данные,
* разная стоимость ошибок,
* ресурс ограничен,
* и систему нужно **реально внедрять**, а не показывать в ноутбуке.

Я сознательно выстраивал работу так, как делал бы это в реальном производственном проекте:

**decision → метрики → модель → политика действий → риски → прод-мышление**.

---

## 1. Бизнес-контекст и реальная проблема

Есть поток стальных пластин.
Каждая пластина имеет **один доминирующий тип дефекта**
(в этом датасете представлены только дефектные случаи).

Типы дефектов:

* Pastry
* Z_Scratch
* K_Scatch
* Stains
* Dirtiness
* Bumps
* Other_Faults

### Ключевая бизнес-проблема

В реальности **невозможно проверять каждую пластину**:

* ручной контроль дорогой, сложный и долгий,
* он замедляет поток,
* он ограничен людьми и оборудованием.

При этом **пропуск критичного дефекта** может стоить:

* рекламаций,
* переделки партии,
* потери клиента,
* потери возможности переработки,
* иногда — репутационных рисков.

Следовательно, задача формулируется не как:

> “Какой дефект у пластины?”

А как:

> **Какие пластины нужно проверить в первую очередь, если мы можем проверить только ограниченную долю потока?**

---

## 2. Decision framing: ML-задача как система решений

### 2.1. Что именно решает система

Система должна **не просто предсказать класс**, а **поддержать решение**.

Возможные действия:

* **PASS** — пропустить без доп. действий
* **QC_CHECK** — отправить на дополнительную проверку
* **HOLD / REWORK** — удержать или отправить на переработку

Ключевое ограничение:

> **QC_CHECK — ограниченный ресурс**.

Поэтому задача естественно превращается в **triage / ранжирование**:

> кого проверять в первую очередь.

---

## 2.2. Почему accuracy / macro-F1 — недостаточно

Ошибки **неравнозначны**:

* FN по критичному дефекту — дорого,
* FP (лишняя проверка) — тоже стоит денег, но обычно дешевле.

Если оптимизировать только accuracy или macro-F1, можно получить:

* “красивые” цифры,
* и при этом **плохие решения для бизнеса**.

Поэтому я сразу разделил:

* **ML-качество** — вспомогательное,
* **Decision-качество** — главное.

---

## 3. Формализация бизнес-метрик

Я зафиксировал, что в этом кейсе важны именно **decision-метрики**.

### Основные метрики

1. **Recall по критичным дефектам**
   Насколько хорошо ловятся дефекты, которые считаются опасными.

2. **Recall@K (Hit-rate@K)**
   Если мы проверяем top-K% по риску — какую долю критичных мы поймаем.

3. **Lift vs random**
   Насколько triage лучше случайного отбора при том же K.

### Важно

> **K — не параметр модели.**
> K — это **ограничение бизнеса** (мощность QC).

---

## 4. Данные и sanity-check

### 4.1. Общая информация

* 1941 строка
* 27 числовых признаков
* 7 таргет-классов
* таргет задан как one-hot (7 бинарных колонок)

### 4.2. Проверка таргета

* В каждой строке ровно **один активный класс**
* Это **multiclass**, а не multilabel

### 4.3. Распределение классов

Классы заметно несбалансированы:

* Other_Faults — ~35%
* Bumps — ~21%
* K_Scatch — ~20%
* Z_Scratch — ~10%
* Pastry — ~8%
* Stains — ~4%
* Dirtiness — ~3%

Imbalance ratio ≈ **12×**.

### Вывод

* macro-метрики обязательны,
* редкие классы легко “теряются”,
* деревья и бустинг предпочтительнее линейных моделей.

---

## 5. Технические особенности данных

### Корреляции

Обнаружены фактически дублирующие признаки:

* TypeOfSteel_A300 ↔ TypeOfSteel_A400
* Y_Minimum ↔ Y_Maximum
* X_Minimum ↔ X_Maximum (почти)

Для деревьев это некритично,
для линейных моделей — риск.

### Выбросы

Много выбросов по:

* Sum_of_Luminosity
* Pixels_Areas
* X_Perimeter
* и др.

Это ещё один аргумент в пользу деревьев.

---

## 6. Валидация

Использовано:

* **Stratified train/test split (80/20)**
* фиксированный random_state

Почему так:

* датасет небольшой,
* нет информации о партиях или времени,
* стратификация обязательна из-за дисбаланса.

Явно зафиксирован риск:

> В реальном производстве потребуется group / time validation.

---

## 7. Baseline модели (ML-слой)

### 7.1. Logistic Regression

Использовалась как sanity-check.

Результат:

* macro-F1 заметно ниже,
* плохой recall по Other_Faults,
* слабое разделение сложных классов.

**Вывод:** линейная модель не подходит как основа.

---

### 7.2. Random Forest

* высокий macro-F1,
* хороший recall по большинству классов,
* устойчив к выбросам.

---

### 7.3. CatBoost

* macro-F1 сопоставим с RF,
* очень хороший recall по K_Scatch и Z_Scratch,
* лучше управляет сложными нелинейностями.

**Вывод:** ML-качество достаточное, дальше важнее decision-слой.

---

## 8. Критичность как гипотеза

В датасете **нет информации о реальной стоимости дефектов**.
Поэтому критичность — **гипотеза**, а не истина.

Рассмотрены сценарии:

1. **crit_broad**
   Other_Faults + K_Scatch + Z_Scratch
   → слишком широко, triage теряет смысл.

2. **crit_unknown**
   Только Other_Faults
   → “неизвестные / потенциально опасные”.

3. **crit_scratches (основной)**
   K_Scatch + Z_Scratch
   → дефекты поверхности, реалистичный кейс.

4. **crit_rare**
   Stains + Dirtiness
   → редкие, но специфичные дефекты.

---

## 9. Почему выбран crit_scratches

* доля ≈ 30% — не слишком мало и не слишком много,
* понятный инженерный смысл,
* стабильный lift,
* остаётся пространство для анализа ошибок.

Это **осознанный выбор**, а не тот что дает больше цифр.

---

## 10. Policy: как работает triage

### Risk-score

Для каждой пластины:

risk_scratches = P(K_Scatch) + P(Z_Scratch)

### Decision policy

* отсортировать пластины по risk,
* QC проверяет **top-K%**.

Без порогов. Только ранжирование.

---

## 11. Результаты triage

Для CatBoost:

* K = 5% → Recall@K ≈ 0.17 → lift ≈ 3.4×
* K = 10% → Recall@K ≈ 0.33 → lift ≈ 3.3×
* K = 15% → Recall@K ≈ 0.50 → lift ≈ 3.3×

### Интерпретация

При фиксированном ресурсе QC система:

* вытягивает критичные дефекты в начало очереди,
* работает существенно лучше случайного отбора,
* масштабируется с ростом K предсказуемо.

---

## 12. Cost-thinking и роль K

Использовалась условная cost-модель, чтобы:

* проверить чувствительность,
* понять направление оптимизации.

Важно:

> Без ограничений экономическая оптимизация всегда будет толкать K вверх.

Это не ошибка, а следствие:

* пропуск критичного дороже проверки.

### Ключевой вывод

В реальности:

* **K задаётся мощностью QC**,
* задача модели — **максимизировать Recall@K при фиксированном K**.

---

## 13. Проверка очевидных улучшений

### 13.1. Binary модель critical vs non-critical

Ожидание: ranking станет лучше.

Факт:

* Recall@K и lift практически совпадают,
* multiclass-risk уже достаточно хорош.

---

### 13.2. Усиление весов критичных классов

Ожидание: критичные поднимутся выше.

Факт:

* при K ≤ 10% эффекта нет,
* при K = 15% эффект минимальный.

### Вывод

Я оставил **более простое решение**, потому что оно не хуже.
Это осознанный выбор.

---

## 14. Error analysis

### 14.1. Опасные FN

Часть K_Scatch / Z_Scratch:

* уходит в Other_Faults,
* с очень низким risk.

Это главный риск triage.

---

### 14.2. Queue eaters

Некоторые Other_Faults:

* имеют высокий risk,
* попадают в top-K,
* “съедают” ресурс QC.

Эффект нормальный, но требует мониторинга.

---

## 15. Feature importance и интерпретация

Топ-фичи:

* Length_of_Conveyer
* Steel_Plate_Thickness
* Edges_Index
* Square_Index
* Luminosity / geometry признаки

Интерпретация:

* модель опирается на параметры процесса,
* геометрию и характеристики поверхности,
* а не на шум.

---

## 16. Production-мышление

При внедрении в реальности я бы зафиксировал:

### Monitoring

* распределение risk,
* долю критичных в top-K,
* долю Other_Faults в top-K,
* дрейф ключевых фич.

### Fallback

* временное увеличение K,
* ручной контроль,
* пересмотр сценария критичности.

---

## 17. Главный итог кейса

В этом кейсе я показал, как:

* начинать с **решения**, а не с модели,
* работать с **ограничениями**,
* измерять **ценность**, а не только качество,
* честно проверять улучшения,
* думать о внедрении и рисках.